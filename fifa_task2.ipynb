{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae4e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f57730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigate to dataset in order to extract csv files\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "folder_path = os.path.join(curr_dir, \"fifa_dataset\")\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "male_files = [file for file in files if file.startswith('p') and file.endswith('.csv')]\n",
    "\n",
    "appName = \"FIFA Dataset Ingestion\"\n",
    "master = \"local\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f417a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:27:17 WARN Utils: Your hostname, Nithanths-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.101 instead (on interface en0)\n",
      "23/11/16 01:27:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/16 01:27:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sofifa_id: integer (nullable = true)\n",
      " |-- player_url: string (nullable = true)\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- long_name: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: integer (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: integer (nullable = true)\n",
      " |-- club_loaned_from: string (nullable = true)\n",
      " |-- club_joined: date (nullable = true)\n",
      " |-- club_contract_valid_until: integer (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- nation_team_id: double (nullable = true)\n",
      " |-- nation_position: string (nullable = true)\n",
      " |-- nation_jersey_number: integer (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- real_face: string (nullable = true)\n",
      " |-- release_clause_eur: string (nullable = true)\n",
      " |-- player_tags: string (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: string (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- goalkeeping_speed: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- player_face_url: string (nullable = true)\n",
      " |-- club_logo_url: string (nullable = true)\n",
      " |-- club_flag_url: string (nullable = true)\n",
      " |-- nation_logo_url: string (nullable = true)\n",
      " |-- nation_flag_url: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- unique_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start spark session - throttle memory threshold to 6GB\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SystemsToolChains\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# combine all years csv for male players\n",
    "master_df = None\n",
    "for csv_file in male_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "    current_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    year = f\"20{csv_file.split('_')[-1].split('.')[0]}\"\n",
    "    current_df = current_df.withColumn(\"year\", lit(year).cast(\"int\"))\n",
    "    if not master_df:\n",
    "        master_df = current_df\n",
    "    else:\n",
    "        master_df = master_df.union(current_df)\n",
    "\n",
    "master_df = master_df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "\n",
    "master_df.printSchema()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15547105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: integer (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_joined: date (nullable = true)\n",
      " |-- club_contract_valid_until: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: string (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- unique_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns with over 50% null values\n",
    "null_percentage = [(col_name, (master_df.filter(col(col_name).isNull()).count() / master_df.count()) * 100.0)\n",
    "                   for col_name in master_df.columns]\n",
    "\n",
    "drop_50 = [col_name for col_name, percentage in null_percentage if percentage > 50.0]\n",
    "master_df = master_df.drop(*drop_50)\n",
    "\n",
    "\n",
    "# drop uninformative features\n",
    "irrelevant_features = [\n",
    "    'sofifa_id', 'player_url', 'short_name', 'long_name', 'player_positions', 'dob',\n",
    "    'player_face_url', 'club_name', 'club_logo_url', 'club_flag_url', 'nation_team_id', \n",
    "    'nation_logo_url', 'nation_flag_url', 'club_jersey_number', \n",
    "    'club_loaned_from', 'release_clause_eur', 'player_tags', \n",
    "    'player_traits', 'real_face', 'nationality_id', \n",
    "    'nation_jersey_number', 'year' \n",
    "] \n",
    "\n",
    "master_df = master_df.drop(*irrelevant_features)\n",
    "\n",
    "\n",
    "master_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ad7d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|body_type|\n",
      "+---------+\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Normal|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "|   Unique|\n",
      "+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean up body type string feature\n",
    "master_df = master_df.withColumn(\"body_type\", trim(regexp_replace(\"body_type\", r'\\(.*\\)', '')))\n",
    "master_df.select('body_type').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfde0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|contract_length|\n",
      "+---------------+\n",
      "|              2|\n",
      "|              9|\n",
      "|              2|\n",
      "|              8|\n",
      "|             10|\n",
      "|              9|\n",
      "|              4|\n",
      "|             12|\n",
      "|             11|\n",
      "|             14|\n",
      "|              7|\n",
      "|             14|\n",
      "|              8|\n",
      "|             10|\n",
      "|             12|\n",
      "+---------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, year\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def calculate_contract_length_spark(df):\n",
    "    \"\"\"\n",
    "    Calculate the length of the contract \n",
    "    'club_joined' is a date from which the year will be extracted.\n",
    "    'club_contract_valid_until' is an integer representing a year.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define UDF to calculate total contract length\n",
    "    def contract_length_udf(joined_year, valid_until_year):\n",
    "        if joined_year is not None and valid_until_year is not None:\n",
    "            return valid_until_year - joined_year\n",
    "        else:\n",
    "            return None  \n",
    "\n",
    "    # Register UDF to run\n",
    "    contract_length_udf_spark = udf(contract_length_udf, IntegerType())\n",
    "\n",
    "    # Extract year from 'club_joined' and cast 'club_contract_valid_until' to integer for calculation\n",
    "    df = df.withColumn('joined_year', year(col('club_joined'))) \\\n",
    "           .withColumn('valid_until_year', col('club_contract_valid_until').cast(IntegerType()))\n",
    "\n",
    "    df = df.withColumn('contract_length', contract_length_udf_spark(col('joined_year'), col('valid_until_year')))\n",
    "\n",
    "    df = df.drop('joined_year', 'valid_until_year', 'club_joined', 'club_contract_valid_until')\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "master_df = calculate_contract_length_spark(master_df)\n",
    "\n",
    "master_df.select('contract_length').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7850047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:28:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+---+---------+---------+------------+--------------------+------------+-------------+----------------+--------------+---------+-----------+------------------------+-------------+---------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+\n",
      "|overall|potential|value_eur|wage_eur|age|height_cm|weight_kg|club_team_id|         league_name|league_level|club_position|nationality_name|preferred_foot|weak_foot|skill_moves|international_reputation|    work_rate|body_type|pace|shooting|passing|dribbling|defending|physic|attacking_crossing|attacking_finishing|attacking_heading_accuracy|attacking_short_passing|attacking_volleys|skill_dribbling|skill_curve|skill_fk_accuracy|skill_long_passing|skill_ball_control|movement_acceleration|movement_sprint_speed|movement_agility|movement_reactions|movement_balance|power_shot_power|power_jumping|power_stamina|power_strength|power_long_shots|mentality_aggression|mentality_interceptions|mentality_positioning|mentality_vision|mentality_penalties|mentality_composure|defending_marking_awareness|defending_standing_tackle|defending_sliding_tackle|goalkeeping_diving|goalkeeping_handling|goalkeeping_kicking|goalkeeping_positioning|goalkeeping_reflexes| ls| st| rs| lw| lf| cf| rf| rw|lam|cam|ram| lm|lcm| cm|rcm| rm|lwb|ldm|cdm|rdm|rwb| lb|lcb| cb|rcb| rb| gk|unique_id|contract_length|\n",
      "+-------+---------+---------+--------+---+---------+---------+------------+--------------------+------------+-------------+----------------+--------------+---------+-----------+------------------------+-------------+---------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+\n",
      "|     93|       93|    7.8E7|320000.0| 34|      170|       72|        73.0|      French Ligue 1|           1|           RW|       Argentina|          Left|        4|          4|                       5|   Medium/Low|   Unique|  85|      92|     91|       95|       34|    65|                85|                 95|                        70|                     91|               88|             96|         93|               94|                91|                96|                   91|                   80|              91|                94|              95|              86|           68|           72|            69|              94|                  44|                     40|                   93|              95|                 75|                 96|                         20|                       35|                      24|                 6|                  11|                 15|                     14|                   8| 92| 92| 92| 92| 93| 93| 93| 92| 93| 93| 93| 93| 90| 90| 90| 93| 69| 67| 67| 67| 69| 64| 53| 53| 53| 64| 22|        0|              2|\n",
      "|     92|       92|  1.195E8|270000.0| 32|      185|       81|        21.0|German 1. Bundesliga|           1|           ST|          Poland|         Right|        4|          4|                       5|  High/Medium|   Unique|  78|      92|     79|       86|       44|    82|                71|                 95|                        90|                     85|               89|             85|         79|               85|                70|                88|                   77|                   79|              77|                93|              82|              90|           85|           76|            86|              87|                  81|                     49|                   95|              81|                 90|                 88|                         35|                       42|                      19|                15|                   6|                 12|                      8|                  10| 92| 92| 92| 85| 88| 88| 88| 85| 89| 89| 89| 87| 83| 83| 83| 87| 67| 69| 69| 69| 67| 64| 63| 63| 63| 64| 22|        1|              9|\n",
      "|     91|       91|    4.5E7|270000.0| 36|      187|       83|        11.0|English Premier L...|           1|           ST|        Portugal|         Right|        4|          5|                       5|     High/Low|   Unique|  87|      94|     80|       88|       34|    75|                87|                 95|                        90|                     80|               86|             88|         81|               84|                77|                88|                   85|                   88|              86|                94|              74|              94|           95|           77|            77|              93|                  63|                     29|                   95|              76|                 88|                 95|                         24|                       32|                      24|                 7|                  11|                 15|                     14|                  11| 91| 91| 91| 88| 89| 89| 89| 88| 89| 89| 89| 89| 81| 81| 81| 89| 66| 62| 62| 62| 66| 63| 56| 56| 56| 63| 23|        2|              2|\n",
      "|     91|       91|   1.29E8|270000.0| 29|      175|       68|        73.0|      French Ligue 1|           1|           LW|          Brazil|         Right|        5|          5|                       5|  High/Medium|   Unique|  91|      83|     86|       94|       37|    63|                85|                 83|                        63|                     86|               86|             95|         88|               87|                81|                95|                   93|                   89|              96|                89|              84|              80|           64|           81|            53|              81|                  63|                     37|                   86|              90|                 93|                 93|                         35|                       32|                      29|                 9|                   9|                 15|                     15|                  11| 86| 86| 86| 90| 88| 88| 88| 90| 91| 91| 91| 91| 85| 85| 85| 91| 70| 66| 66| 66| 70| 65| 53| 53| 53| 65| 23|        3|              8|\n",
      "|     91|       91|  1.255E8|350000.0| 30|      181|       70|        10.0|English Premier L...|           1|          RCM|         Belgium|         Right|        5|          4|                       4|    High/High|   Unique|  76|      86|     93|       88|       64|    78|                94|                 82|                        55|                     94|               82|             88|         85|               83|                93|                91|                   76|                   76|              79|                91|              78|              91|           63|           89|            74|              91|                  76|                     66|                   88|              94|                 83|                 89|                         68|                       65|                      53|                15|                  13|                  5|                     10|                  13| 86| 86| 86| 88| 87| 87| 87| 88| 91| 91| 91| 91| 91| 91| 91| 91| 82| 83| 83| 83| 82| 78| 72| 72| 72| 78| 24|        4|             10|\n",
      "|     91|       93|   1.12E8|130000.0| 28|      188|       87|       240.0|Spain Primera Div...|           1|           GK|        Slovenia|         Right|        3|          1|                       5|Medium/Medium|   Unique|null|    null|   null|     null|     null|  null|                13|                 11|                        15|                     43|               13|             12|         13|               14|                40|                30|                   43|                   60|              67|                88|              49|              59|           78|           41|            78|              12|                  34|                     19|                   11|              65|                 11|                 68|                         27|                       12|                      18|                87|                  92|                 78|                     90|                  90| 36| 36| 36| 32| 35| 35| 35| 32| 41| 41| 41| 38| 41| 41| 41| 38| 35| 39| 39| 39| 35| 35| 36| 36| 36| 35| 92|        5|              9|\n",
      "|     91|       95|   1.94E8|230000.0| 22|      182|       73|        73.0|      French Ligue 1|           1|           ST|          France|         Right|        4|          5|                       4|     High/Low|   Unique|  97|      88|     80|       92|       36|    77|                78|                 93|                        72|                     85|               83|             93|         80|               69|                71|                91|                   97|                   97|              92|                93|              83|              86|           78|           88|            77|              82|                  62|                     38|                   92|              82|                 79|                 88|                         26|                       34|                      32|                13|                   5|                  7|                     11|                   6| 92| 92| 92| 90| 90| 90| 90| 90| 92| 92| 92| 92| 84| 84| 84| 92| 70| 66| 66| 66| 70| 66| 57| 57| 57| 66| 21|        6|              4|\n",
      "|     90|       90|   1.35E7| 86000.0| 35|      193|       93|        21.0|German 1. Bundesliga|           1|           GK|         Germany|         Right|        4|          1|                       5|Medium/Medium|   Unique|null|    null|   null|     null|     null|  null|                15|                 13|                        25|                     60|               11|             30|         14|               11|                68|                46|                   54|                   60|              51|                87|              35|              68|           77|           43|            80|              16|                  29|                     30|                   12|              70|                 47|                 70|                         17|                       10|                      11|                88|                  88|                 91|                     89|                  88| 43| 43| 43| 40| 43| 43| 43| 40| 50| 50| 50| 47| 53| 53| 53| 47| 40| 46| 46| 46| 40| 38| 37| 37| 37| 38| 90|        7|             12|\n",
      "|     90|       92|    9.9E7|250000.0| 29|      187|       85|       241.0|Spain Primera Div...|           1|           GK|         Germany|         Right|        4|          1|                       4|Medium/Medium|   Unique|null|    null|   null|     null|     null|  null|                18|                 14|                        11|                     61|               14|             21|         18|               12|                63|                30|                   38|                   50|              39|                86|              43|              66|           79|           35|            78|              10|                  43|                     22|                   11|              70|                 25|                 70|                         25|                       13|                      10|                88|                  85|                 88|                     88|                  90| 38| 38| 38| 35| 38| 38| 38| 35| 45| 45| 45| 42| 48| 48| 48| 42| 36| 44| 44| 44| 36| 34| 36| 36| 36| 34| 91|        8|             11|\n",
      "|     90|       90|  1.295E8|240000.0| 27|      188|       89|        18.0|English Premier L...|           1|           ST|         England|         Right|        5|          3|                       4|    High/High|   Unique|  70|      91|     83|       83|       47|    83|                80|                 94|                        86|                     85|               88|             83|         83|               65|                86|                85|                   65|                   74|              71|                92|              70|              91|           79|           83|            85|              86|                  80|                     44|                   94|              87|                 91|                 91|                         50|                       36|                      38|                 8|                  10|                 11|                     14|                  11| 90| 90| 90| 84| 86| 86| 86| 84| 88| 88| 88| 87| 85| 85| 85| 87| 70| 71| 71| 71| 70| 67| 64| 64| 64| 67| 23|        9|             14|\n",
      "|     90|       90|    1.0E8|230000.0| 30|      168|       70|         5.0|English Premier L...|           1|          RCM|          France|         Right|        3|          2|                       4|  Medium/High|   Unique|  78|      66|     75|       82|       87|    83|                68|                 65|                        54|                     82|               56|             79|         49|               49|                79|                81|                   82|                   75|              82|                93|              92|              71|           77|           97|            72|              65|                  93|                     91|                   72|              78|                 54|                 84|                         90|                       93|                      86|                15|                  12|                 10|                      7|                  10| 75| 75| 75| 77| 77| 77| 77| 77| 81| 81| 81| 82| 85| 85| 85| 82| 88| 90| 90| 90| 88| 88| 87| 87| 87| 88| 23|       10|              7|\n",
      "|     89|       89|    6.6E7|350000.0| 33|      185|       81|       243.0|Spain Primera Div...|           1|           CF|          France|         Right|        4|          4|                       4|Medium/Medium|   Normal|  76|      86|     81|       87|       39|    77|                75|                 90|                        89|                     86|               86|             87|         81|               73|                73|                90|                   77|                   75|              79|                91|              72|              85|           79|           78|            82|              79|                  63|                     39|                   90|              87|                 84|                 90|                         43|                       24|                      18|                13|                  11|                  5|                      5|                   7| 89| 89| 89| 85| 87| 87| 87| 85| 89| 89| 89| 87| 83| 83| 83| 87| 66| 65| 65| 65| 66| 62| 58| 58| 58| 62| 21|       11|             14|\n",
      "|     89|       91|   8.55E7|250000.0| 29|      199|       96|       243.0|Spain Primera Div...|           1|           GK|         Belgium|          Left|        3|          1|                       4|Medium/Medium|   Unique|null|    null|   null|     null|     null|  null|                14|                 14|                        13|                     33|               12|             13|         19|               20|                35|                23|                   42|                   52|              62|                84|              45|              56|           68|           38|            70|              17|                  23|                     15|                   13|              44|                 27|                 66|                         20|                       18|                      16|                84|                  89|                 74|                     86|                  88| 34| 34| 34| 29| 31| 31| 31| 29| 35| 35| 35| 34| 35| 35| 35| 34| 32| 34| 34| 34| 32| 32| 32| 32| 32| 32| 89|       12|              8|\n",
      "|     89|       89|   1.04E8|220000.0| 28|      183|       78|        18.0|English Premier L...|           1|           LW|  Korea Republic|         Right|        5|          4|                       4|    High/High|   Unique|  88|      87|     82|       86|       43|    69|                83|                 88|                        68|                     84|               78|             87|         85|               74|                74|                84|                   85|                   90|              86|                91|              78|              88|           60|           88|            64|              89|                  62|                     39|                   91|              83|                 73|                 89|                         50|                       34|                      33|                11|                  13|                 13|                      6|                  10| 88| 88| 88| 87| 87| 87| 87| 87| 89| 89| 89| 89| 83| 83| 83| 89| 71| 67| 67| 67| 71| 67| 57| 57| 57| 67| 22|       13|             10|\n",
      "|     89|       89|    8.8E7|310000.0| 29|      185|       84|       243.0|Spain Primera Div...|           1|          CDM|          Brazil|         Right|        3|          2|                       3|    High/High|   Unique|  65|      73|     76|       73|       86|    90|                58|                 64|                        79|                     84|               62|             69|         62|               74|                84|                79|                   60|                   69|              61|                87|              66|              88|           87|           89|            90|              81|                  91|                     87|                   75|              80|                 66|                 84|                         85|                       88|                      87|                13|                  14|                 16|                     12|                  12| 79| 79| 79| 72| 76| 76| 76| 72| 79| 79| 79| 77| 85| 85| 85| 77| 83| 89| 89| 89| 83| 84| 89| 89| 89| 84| 24|       14|             12|\n",
      "+-------+---------+---------+--------+---+---------+---------+------------+--------------------+------------+-------------+----------------+--------------+---------+-----------+------------------------+-------------+---------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing skill columns by handling all suffixes\n",
    "def combine_skill_values(value):\n",
    "    if isinstance(value, str):\n",
    "        if '+' in value:\n",
    "            parts = value.split('+')\n",
    "            return int(parts[0]) + int(parts[1])\n",
    "        elif '-' in value:\n",
    "            parts = value.split('-')\n",
    "            return int(parts[0]) - int(parts[1])\n",
    "        else:\n",
    "            return int(value)\n",
    "    return value\n",
    "\n",
    "skill_columns = [\n",
    "    \"ls\", \"st\", \"rs\", \"lw\", \"lf\", \"cf\", \"rf\", \"rw\",\n",
    "    \"lam\", \"cam\", \"ram\", \"lm\", \"lcm\", \"cm\", \"rcm\", \"rm\",\n",
    "    \"lwb\", \"ldm\", \"cdm\", \"rdm\", \"rwb\", \"lb\", \"lcb\", \"cb\",\n",
    "    \"rcb\", \"rb\", \"gk\"\n",
    "]\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "map_skill = udf(combine_skill_values, IntegerType())\n",
    "\n",
    "\n",
    "for col_name in skill_columns:\n",
    "    master_df = master_df.withColumn(col_name, map_skill(master_df[col_name]))\n",
    "master_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential', 'value_eur', 'wage_eur', 'age', 'height_cm', 'weight_kg', 'club_team_id', 'league_level', 'weak_foot', 'skill_moves', 'international_reputation', 'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic', 'attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control', 'movement_acceleration', 'movement_sprint_speed', 'movement_agility', 'movement_reactions', 'movement_balance', 'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots', 'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties', 'mentality_composure', 'defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk', 'contract_length']\n",
      "['league_name', 'club_position', 'nationality_name', 'preferred_foot', 'work_rate', 'body_type']\n"
     ]
    }
   ],
   "source": [
    "# clean up the categorical columns and visualize both numerical and categorical features\n",
    "master_df = master_df.withColumn('mentality_composure', col('mentality_composure').cast(IntegerType()))\n",
    "numerical_cols = [f.name for f in master_df.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, FloatType))]\n",
    "categorical_cols = [f.name for f in master_df.schema.fields if isinstance(f.dataType, (StringType))]\n",
    "numerical_cols.remove(\"overall\")\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in league_name:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spain Primera Division', 'UAE Arabian Gulf League', 'English Premier League', 'Turkish Süper Lig', 'Swiss Super League', 'Korean K League 1', 'French Ligue 1', 'Ukrainian Premier League', 'Croatian Prva HNL', None, 'Scottish Premiership', 'Romanian Liga I', 'Chinese Super League', 'Greek Super League', 'French Ligue 2', 'South African Premier Division', 'Mexican Liga MX', 'Portuguese Liga ZON SAGRES', 'Danish Superliga', 'German 1. Bundesliga', 'Swedish Allsvenskan', 'German 2. Bundesliga', 'Campeonato Brasileiro Série A', 'Russian Premier League', 'Holland Eredivisie', 'Paraguayan Primera División', 'English League Championship', 'Polish T-Mobile Ekstraklasa', 'German 3. Bundesliga', 'Liga de Fútbol Profesional Boliviano', 'Chilian Campeonato Nacional', 'Cypriot First Division', 'Japanese J. League Division 1', 'Hungarian Nemzeti Bajnokság I', 'Austrian Football Bundesliga', 'USA Major League Soccer', 'Norwegian Eliteserien', 'Belgian Jupiler Pro League', 'Indian Super League', 'Colombian Liga Postobón', 'Italian Serie A', 'Czech Republic Gambrinus Liga', 'Italian Serie B', 'Uruguayan Primera División', 'English League One', 'Spanish Segunda División', 'Australian Hyundai A-League', 'Peruvian Primera División', 'Saudi Abdul L. Jameel League', 'Venezuelan Primera División', 'Ecuadorian Serie A', 'Argentina Primera División', 'Finnish Veikkausliiga', 'English League Two', 'English National League', 'Rep. Ireland Airtricity League', 'Rest of World', 'Campeonato Brasileiro Série B', 'Scottish Championship']\n",
      "59\n",
      "\n",
      "\n",
      "Unique values in club_position:\n",
      "['RF', 'LWB', 'LCM', 'LM', 'RDM', 'RES', 'LF', None, 'CAM', 'RAM', 'LB', 'LW', 'RCM', 'GK', 'RB', 'RS', 'LCB', 'CM', 'RW', 'RCB', 'CDM', 'LS', 'CB', 'SUB', 'RWB', 'CF', 'RM', 'LAM', 'LDM', 'ST']\n",
      "30\n",
      "\n",
      "\n",
      "Unique values in nationality_name:\n",
      "[\"Côte d'Ivoire\", 'Chad', 'Russia', 'Paraguay', 'Chinese Taipei', 'Senegal', 'Sweden', 'Philippines', 'Fiji', 'Turkey', 'Malawi', 'Germany', 'Comoros', 'France', 'Greece', 'Kosovo', 'Montserrat', 'Algeria', 'Togo', 'Congo DR', 'Equatorial Guinea', 'Slovakia', 'Argentina', 'Wales', 'Belgium', 'Angola', 'Ecuador', 'Albania', 'Madagascar', 'Finland', 'Ghana', 'Peru', 'Benin', 'Sierra Leone', 'United States', 'Curacao', 'Chile', 'Croatia', 'Burundi', 'Nigeria', 'Bolivia', 'Korea Republic', 'Gabon', 'Italy', 'Suriname', 'Lithuania', 'Norway', 'Spain', 'Cuba', 'Denmark', 'Central African Republic', 'Iran', 'Congo', 'Thailand', 'Morocco', 'Panama', 'Ukraine', 'Venezuela', 'Israel', 'Iceland', 'Saint Kitts and Nevis', 'Cyprus', 'Uruguay', 'Mexico', 'Montenegro', 'Zimbabwe', 'Georgia', 'Libya', 'Armenia', 'Tunisia', 'Liberia', 'Syria', 'Honduras', 'Trinidad and Tobago', 'Saudi Arabia', 'Namibia', 'Switzerland', 'Zambia', 'Jamaica', 'United Arab Emirates', 'Guinea', 'Canada', 'Cape Verde Islands', 'North Macedonia', 'Uzbekistan', 'Czech Republic', 'Mozambique', 'Brazil', 'Gambia', 'Kenya', 'Slovenia', 'Antigua and Barbuda', 'Dominican Republic', 'Japan', 'Tanzania', 'Republic of Ireland', 'Luxembourg', 'New Zealand', 'England', 'Bosnia and Herzegovina', 'China PR', 'Haiti', 'Poland', 'Portugal', 'Cameroon', 'Australia', 'Romania', 'Bulgaria', 'Austria', 'Egypt', 'Costa Rica', 'Kazakhstan', 'Serbia', 'Burkina Faso', 'South Africa', 'Bermuda', 'Scotland', 'Colombia', 'Northern Ireland', 'Hungary', 'Guinea Bissau', 'Moldova', 'Netherlands', 'Mali', 'Guyana', 'Eritrea', 'Iraq', 'Jordan', 'Sudan', 'India', 'Belarus', 'Malta', 'Puerto Rico', 'Andorra', 'Mauritania', 'Niger', 'Korea DPR', 'Guam', 'Azerbaijan', 'Grenada', 'Uganda', 'Ethiopia', 'Latvia', 'Saint Lucia', 'Kyrgyzstan', 'Faroe Islands', 'Lebanon', 'Papua New Guinea', 'El Salvador', 'Mauritius', 'Malaysia', 'Afghanistan', 'Barbados', 'Bhutan', 'Hong Kong', 'Palestine', 'Gibraltar', 'Estonia', 'Guatemala', 'South Sudan', 'Belize', 'Vietnam', 'Indonesia', 'New Caledonia', 'São Tomé e Príncipe', 'Rwanda', 'Liechtenstein', 'Aruba', 'Macau', 'Nicaragua', 'Bahrain', 'Kuwait', 'Oman', 'Qatar', 'Somalia', 'Sri Lanka', 'San Marino', 'Brunei Darussalam', 'Turkmenistan', 'Swaziland', 'Saint Vincent and the Grenadines', 'Tajikistan', 'Cambodia']\n",
      "183\n",
      "\n",
      "\n",
      "Unique values in preferred_foot:\n",
      "['Left', 'Right']\n",
      "2\n",
      "\n",
      "\n",
      "Unique values in work_rate:\n",
      "['Medium/Medium', 'High/Medium', 'Low/High', 'Low/Low', 'High/High', 'Medium/Low', 'High/Low', 'Medium/High', 'Low/Medium']\n",
      "9\n",
      "\n",
      "\n",
      "Unique values in body_type:\n",
      "['Stocky', 'Lean', 'Unique', 'Normal']\n",
      "4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking unique count of categorical columns to decide on encoding strategy\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"Unique values in {col_name}:\")\n",
    "    unique_values = master_df.select(col_name).distinct().collect()\n",
    "    unique_values_list = [row[col_name] for row in unique_values]\n",
    "    print(unique_values_list)\n",
    "    print(len(unique_values_list))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f10224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def preprocess_and_split_spark_df(df, target_col_name, numerical_cols, categorical_cols, train_ratio=0.7, val_ratio=0.15):\n",
    "    # Update categorical_cols to exclude features too large for one hot encoding\n",
    "    categorical_cols = [col for col in categorical_cols if col not in ['nationality_name', 'league_name']]\n",
    "\n",
    "    # Splitting into features and target\n",
    "    feature_df = df.drop(target_col_name)\n",
    "    target_df = df.select(\"unique_id\", target_col_name)\n",
    "\n",
    "    # Impute missing values in numerical columns\n",
    "    numerical_imputer = Imputer(inputCols=numerical_cols, outputCols=numerical_cols, strategy=\"mean\")\n",
    "\n",
    "    # normalize numerical features\n",
    "    numerical_assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features\")\n",
    "    numerical_scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical_features\")\n",
    "\n",
    "    # Impute missing values in categorical columns with the mode of feature\n",
    "    for col_name in categorical_cols:\n",
    "        mode = feature_df.groupBy(col_name).count().orderBy('count', ascending=False).first()[0]\n",
    "        feature_df = feature_df.withColumn(col_name, when(col(col_name).isNull(), mode).otherwise(col(col_name)))\n",
    "\n",
    "    # StringIndexer and OneHotEncoder for categorical columns\n",
    "    indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\", handleInvalid=\"keep\") for col_name in categorical_cols]\n",
    "    encoder = OneHotEncoder(inputCols=[col_name + \"_index\" for col_name in categorical_cols],\n",
    "                            outputCols=[col_name + \"_encoded\" for col_name in categorical_cols])\n",
    "\n",
    "    # Feature Hashers for 'nationality_name' and 'league_name'\n",
    "    hasher_nationality = FeatureHasher(inputCols=[\"nationality_name\"], outputCol=\"hashed_nationality\", numFeatures=100)\n",
    "    hasher_league = FeatureHasher(inputCols=[\"league_name\"], outputCol=\"hashed_league\", numFeatures=50)\n",
    "\n",
    "    # Combine all preprocessors into a pipeline\n",
    "    pipeline = Pipeline(stages=[numerical_imputer, numerical_assembler, numerical_scaler] + indexers + [encoder, hasher_nationality, hasher_league])\n",
    "    transformed_feature_df = pipeline.fit(feature_df).transform(feature_df)\n",
    "\n",
    "    # Join the transformed features with the target \n",
    "    joined_df = transformed_feature_df.join(target_df, \"unique_id\")\n",
    "\n",
    "    # Apply VectorAssembler to generate comprehensive feature vectors\n",
    "    assembler_inputs = [col + \"_encoded\" for col in categorical_cols] + [\"scaled_numerical_features\", \"hashed_nationality\", \"hashed_league\"]\n",
    "    feature_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "    final_df = feature_assembler.transform(joined_df).select(\"features\", target_col_name)\n",
    "\n",
    "    \n",
    "    final_df = final_df.drop('unique_id')\n",
    "\n",
    "    # Train-Validation-Test Split\n",
    "    train_df, val_df, test_df = final_df.randomSplit([train_ratio, val_ratio, 1 - train_ratio - val_ratio])\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = preprocess_and_split_spark_df(master_df, 'overall', numerical_cols, categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|overall|\n",
      "+--------------------+-------+\n",
      "|(273,[0,29,31,40,...|     74|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------+\n",
      "|            features|overall|\n",
      "+--------------------+-------+\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "|(273,[0,29,31,40,...|     71|\n",
      "|(273,[0,29,31,40,...|     72|\n",
      "|(273,[0,29,31,40,...|     72|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------+\n",
      "|            features|overall|\n",
      "+--------------------+-------+\n",
      "|(273,[0,29,31,40,...|     73|\n",
      "|(273,[0,29,31,40,...|     73|\n",
      "|(273,[0,29,31,40,...|     71|\n",
      "|(273,[0,29,31,40,...|     71|\n",
      "|(273,[0,29,31,40,...|     70|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample of transformed and preprocessed dataframes\n",
    "train_df.show(5)\n",
    "val_df.show(5)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:29:55 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - Root Mean Squared Error (RMSE): 1.6739712983854318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Root Mean Squared Error (RMSE): 1.6832906478928829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 768:===========================================>           (23 + 6) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Mean Absolute Error (MAE): 1.1994771721117659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Running Random Forest\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='overall')\n",
    "\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "train_predictions_rf = rf_model.transform(train_df)\n",
    "\n",
    "# evaluator for regression with RMSE metric\n",
    "evaluator_rf = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluating model on the training set\n",
    "train_rmse_rf = evaluator_rf.evaluate(train_predictions_rf)\n",
    "print(\"Training Set - Root Mean Squared Error (RMSE):\", train_rmse_rf)\n",
    "\n",
    "# Transform the model on the test dataset\n",
    "test_predictions_rf = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_rmse_rf = evaluator_rf.evaluate(test_predictions_rf)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", test_rmse_rf)\n",
    "\n",
    "# Optionally, evaluate using MAE (Mean Absolute Error)\n",
    "evaluator_mae_rf = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "test_mae_rf = evaluator_mae_rf.evaluate(test_predictions_rf)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", test_mae_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=10, maxDepth=5, Validation RMSE=1.708622260013104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:30:39 WARN DAGScheduler: Broadcasting large task binary with size 1085.1 KiB\n",
      "23/11/16 01:30:39 WARN DAGScheduler: Broadcasting large task binary with size 1292.9 KiB\n",
      "23/11/16 01:30:40 WARN DAGScheduler: Broadcasting large task binary with size 1709.1 KiB\n",
      "23/11/16 01:30:42 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "23/11/16 01:30:44 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=10, maxDepth=10, Validation RMSE=0.9463339504372835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:31:06 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=20, maxDepth=5, Validation RMSE=1.6507870532346285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:31:20 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "23/11/16 01:31:20 WARN DAGScheduler: Broadcasting large task binary with size 1290.9 KiB\n",
      "23/11/16 01:31:21 WARN DAGScheduler: Broadcasting large task binary with size 1706.2 KiB\n",
      "23/11/16 01:31:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "23/11/16 01:31:26 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/11/16 01:31:34 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=20, maxDepth=10, Validation RMSE=0.9170978803514371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:32:08 WARN DAGScheduler: Broadcasting large task binary with size 1079.2 KiB\n",
      "23/11/16 01:32:09 WARN DAGScheduler: Broadcasting large task binary with size 1287.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=40, maxDepth=5, Validation RMSE=1.6348742241498835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:32:24 WARN DAGScheduler: Broadcasting large task binary with size 1079.2 KiB\n",
      "23/11/16 01:32:25 WARN DAGScheduler: Broadcasting large task binary with size 1287.2 KiB\n",
      "23/11/16 01:32:26 WARN DAGScheduler: Broadcasting large task binary with size 1702.4 KiB\n",
      "23/11/16 01:32:27 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "23/11/16 01:32:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/11/16 01:32:40 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "23/11/16 01:33:01 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "23/11/16 01:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1180.3 KiB\n",
      "23/11/16 01:33:29 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "[Stage 908:===========================================>           (23 + 6) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: numTrees=40, maxDepth=10, Validation RMSE=0.9044286239464913\n",
      "Best Parameters: {'numTrees': 40, 'maxDepth': 10}, Best Validation RMSE: 0.9044286239464913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "numTreesList = [10, 20, 40]\n",
    "maxDepthList = [5, 10]\n",
    "\n",
    "# Variables to store the best model's information\n",
    "best_rmse_rf = float(\"inf\")\n",
    "best_model_rf = None\n",
    "best_params_rf = {}\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for numTrees in numTreesList:\n",
    "    for maxDepth in maxDepthList:\n",
    "        # Define the Random Forest model\n",
    "        rf = RandomForestRegressor(featuresCol='features', labelCol='overall', numTrees=numTrees, maxDepth=maxDepth)\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model_rf = rf.fit(train_df)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_predictions_rf = model_rf.transform(val_df)\n",
    "        val_rmse_rf = evaluator_rf.evaluate(val_predictions_rf)\n",
    "\n",
    "        # Log progress\n",
    "        print(f\"Params: numTrees={numTrees}, maxDepth={maxDepth}, Validation RMSE={val_rmse_rf}\")\n",
    "\n",
    "        # Update the best model if the current one is better\n",
    "        if val_rmse_rf < best_rmse_rf:\n",
    "            best_rmse_rf = val_rmse_rf\n",
    "            best_model_rf = model_rf\n",
    "            best_params_rf = {'numTrees': numTrees, 'maxDepth': maxDepth}\n",
    "\n",
    "# Log the best parameters and RMSE\n",
    "print(f\"Best Parameters: {best_params_rf}, Best Validation RMSE: {best_rmse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Best Model RMSE: 0.9169455616192772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 914:=============================================>         (24 + 5) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Best Model MAE: 0.6552045636918004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_predictions_rf = best_model_rf.transform(test_df)\n",
    "test_rmse_rf = evaluator_rf.evaluate(test_predictions_rf)\n",
    "print(\"Test Set - Best Model RMSE:\", test_rmse_rf)\n",
    "\n",
    "# Optionally, evaluate using MAE\n",
    "evaluator_mae_rf = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "test_mae_rf = evaluator_mae_rf.evaluate(test_predictions_rf)\n",
    "print(\"Test Set - Best Model MAE:\", test_mae_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Random Forest Model has 40 trees in its classifier and 10 as the max depth of its trees. The RF model performs really well for regression achieving an RMSE of 0.917 and an MAE of 0.66 on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 01:33:50 WARN Instrumentation: [cc9914aa] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/11/16 01:33:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/16 01:33:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/11/16 01:33:53 WARN Instrumentation: [cc9914aa] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='overall')\n",
    "\n",
    "# Fit the model on the training dataset\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Transform the model on the training dataset to check its performance there\n",
    "train_predictions = lr_model.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - Root Mean Squared Error (RMSE): 1.7255131267280805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Root Mean Squared Error (RMSE): 1.748399764438042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 930:=========================================>             (22 + 7) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Mean Absolute Error (MAE): 1.3628229667579255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define an evaluator for regression with RMSE metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "print(\"Training Set - Root Mean Squared Error (RMSE):\", train_rmse)\n",
    "# Transform the model on the test dataset\n",
    "test_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", test_rmse)\n",
    "\n",
    "# Optionally, you can also evaluate using MAE (Mean Absolute Error)\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "test_mae = evaluator_mae.evaluate(test_predictions)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.001, maxIter=100, RMSE=1.7444107277932375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.001, maxIter=150, RMSE=1.7444107277932561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.01, maxIter=100, RMSE=1.7446970837524292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.01, maxIter=150, RMSE=1.7446970837524405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.1, maxIter=100, RMSE=1.7518591391100211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 990:===============================================>       (25 + 4) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: regParam=0.1, maxIter=150, RMSE=1.7518591391100236\n",
      "Best Params: {'regParam': 0.001, 'maxIter': 100}, Best RMSE: 1.7444107277932375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# perform cross validation\n",
    "evaluator = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "for regParam in [0.001, 0.01, 0.1]:\n",
    "    for maxIter in [100, 150]:\n",
    "       \n",
    "        lr = LinearRegression(featuresCol='features', labelCol='overall', regParam=regParam, maxIter=maxIter)\n",
    "\n",
    "        # Fit the model\n",
    "        model = lr.fit(train_df)\n",
    "\n",
    "        # Perform evaluation on the validation set\n",
    "        val_predictions = model.transform(val_df)\n",
    "        rmse = evaluator.evaluate(val_predictions)\n",
    "\n",
    "        # Log progress\n",
    "        print(f\"Params: regParam={regParam}, maxIter={maxIter}, RMSE={rmse}\")\n",
    "\n",
    "        # Update best model if current model is better\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "            best_params = {'regParam': regParam, 'maxIter': maxIter}\n",
    "\n",
    "# Log best parameters\n",
    "print(f\"Best Params: {best_params}, Best RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Root Mean Squared Error (RMSE): 1.748300325981113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 996:=============================================>         (24 + 5) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Mean Absolute Error (MAE): 1.3628200056802837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the fitted model to make predictions on the test dataset\n",
    "test_predictions = best_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", test_rmse)\n",
    "\n",
    "# Optionally, evaluate using MAE (Mean Absolute Error)\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "test_mae = evaluator_mae.evaluate(test_predictions)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", test_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the Linear Regression model with regularization of 0.001 and max iterations of 100 performed the best during tuning, with a slightly improved RMSE of 1.744. After applying those parameters to the test set for performance, we get a similar RMSE of 1.7488 and MAE of 1.362 on the test set. So Linear Regression overall does well here with minimal error, but not as well as the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4e13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Converting the Spark DF to Tensors (both PyTorch and TensorFlow)\n",
    "train_pandas_df = train_df.toPandas()\n",
    "val_pandas_df = val_df.toPandas()\n",
    "test_pandas_df = test_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/qq0421bn06j2wvt1dcz7ldy40000gn/T/ipykernel_22691/3819948436.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  features = torch.tensor(df['features'].apply(lambda x: x.toArray()).tolist()).float()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# convert a DataFrame to tensors for PyTorch\n",
    "def convert_to_tensors(df):\n",
    "    features = torch.tensor(df['features'].apply(lambda x: x.toArray()).tolist()).float()\n",
    "    targets = torch.tensor(df['overall'].values).float()\n",
    "    return features, targets\n",
    "\n",
    "\n",
    "train_features_tensor, train_targets_tensor = convert_to_tensors(train_pandas_df)\n",
    "val_features_tensor, val_targets_tensor = convert_to_tensors(val_pandas_df)\n",
    "test_features_tensor, test_targets_tensor = convert_to_tensors(test_pandas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# convert a DataFrame to tensors for TensorFlow\n",
    "def convert_to_tf_tensors(df):\n",
    "    features = np.array(df['features'].apply(lambda x: x.toArray()).tolist())\n",
    "    targets = df['overall'].values\n",
    "    return tf.convert_to_tensor(features, dtype=tf.float32), tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "\n",
    "train_features_tensor_tf, train_targets_tensor_tf = convert_to_tf_tensors(train_pandas_df)\n",
    "val_features_tensor_tf, val_targets_tensor_tf = convert_to_tf_tensors(val_pandas_df)\n",
    "test_features_tensor_tf, test_targets_tensor_tf = convert_to_tf_tensors(test_pandas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99552, 273]) torch.Size([99552])\n",
      "torch.Size([21332, 273]) torch.Size([21332])\n",
      "torch.Size([21195, 273]) torch.Size([21195])\n",
      "\n",
      "\n",
      "(99552, 273) (99552,)\n",
      "(21332, 273) (21332,)\n",
      "(21195, 273) (21195,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_tensor.shape, train_targets_tensor.shape)\n",
    "print(val_features_tensor.shape, val_targets_tensor.shape)\n",
    "print(test_features_tensor.shape, test_targets_tensor.shape)\n",
    "print(\"\\n\")\n",
    "print(train_features_tensor_tf.shape, train_targets_tensor_tf.shape)\n",
    "print(val_features_tensor_tf.shape, val_targets_tensor_tf.shape)\n",
    "print(test_features_tensor_tf.shape, test_targets_tensor_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class VanillaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(273, 128)  # 273 input features\n",
    "        self.fc2 = nn.Linear(128, 1)    # Regression task\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 70.0349, Val Loss: 5.3386\n",
      "Epoch [2/100], Train Loss: 4.7723, Val Loss: 4.2843\n",
      "Epoch [3/100], Train Loss: 3.9878, Val Loss: 3.7910\n",
      "Epoch [4/100], Train Loss: 3.5737, Val Loss: 3.4365\n",
      "Epoch [5/100], Train Loss: 3.3788, Val Loss: 3.4007\n",
      "Epoch [6/100], Train Loss: 3.3049, Val Loss: 3.1396\n",
      "Epoch [7/100], Train Loss: 3.2446, Val Loss: 3.2163\n",
      "Epoch [8/100], Train Loss: 3.2138, Val Loss: 3.1080\n",
      "Epoch [9/100], Train Loss: 3.2043, Val Loss: 3.3817\n",
      "Epoch [10/100], Train Loss: 3.1824, Val Loss: 3.1364\n",
      "Epoch [11/100], Train Loss: 3.2109, Val Loss: 3.1842\n",
      "Epoch [12/100], Train Loss: 3.2024, Val Loss: 3.2559\n",
      "Epoch [13/100], Train Loss: 3.1858, Val Loss: 3.1937\n",
      "Epoch [14/100], Train Loss: 3.1980, Val Loss: 3.2746\n",
      "Epoch [15/100], Train Loss: 3.1686, Val Loss: 3.5703\n",
      "Epoch [16/100], Train Loss: 3.1827, Val Loss: 3.0753\n",
      "Epoch [17/100], Train Loss: 3.1719, Val Loss: 3.6027\n",
      "Epoch [18/100], Train Loss: 3.1910, Val Loss: 3.1065\n",
      "Epoch [19/100], Train Loss: 3.1620, Val Loss: 3.0932\n",
      "Epoch [20/100], Train Loss: 3.1966, Val Loss: 3.0828\n",
      "Epoch [21/100], Train Loss: 3.1550, Val Loss: 3.4112\n",
      "Epoch [22/100], Train Loss: 3.1661, Val Loss: 3.9711\n",
      "Epoch [23/100], Train Loss: 3.1615, Val Loss: 3.3455\n",
      "Epoch [24/100], Train Loss: 3.1393, Val Loss: 3.1104\n",
      "Epoch [25/100], Train Loss: 3.1876, Val Loss: 3.2978\n",
      "Epoch [26/100], Train Loss: 3.0971, Val Loss: 3.5355\n",
      "Epoch [27/100], Train Loss: 2.5985, Val Loss: 2.3185\n",
      "Epoch [28/100], Train Loss: 2.3011, Val Loss: 2.3490\n",
      "Epoch [29/100], Train Loss: 2.1131, Val Loss: 1.9694\n",
      "Epoch [30/100], Train Loss: 1.9306, Val Loss: 1.9525\n",
      "Epoch [31/100], Train Loss: 1.8214, Val Loss: 1.7104\n",
      "Epoch [32/100], Train Loss: 1.6519, Val Loss: 1.6468\n",
      "Epoch [33/100], Train Loss: 1.4510, Val Loss: 1.3030\n",
      "Epoch [34/100], Train Loss: 1.2324, Val Loss: 1.1383\n",
      "Epoch [35/100], Train Loss: 1.0938, Val Loss: 1.1781\n",
      "Epoch [36/100], Train Loss: 1.0115, Val Loss: 0.9081\n",
      "Epoch [37/100], Train Loss: 0.9515, Val Loss: 0.8913\n",
      "Epoch [38/100], Train Loss: 0.8706, Val Loss: 0.8013\n",
      "Epoch [39/100], Train Loss: 0.8418, Val Loss: 0.8141\n",
      "Epoch [40/100], Train Loss: 0.8068, Val Loss: 0.8427\n",
      "Epoch [41/100], Train Loss: 0.7824, Val Loss: 0.7745\n",
      "Epoch [42/100], Train Loss: 0.7454, Val Loss: 0.7652\n",
      "Epoch [43/100], Train Loss: 0.7321, Val Loss: 0.7110\n",
      "Epoch [44/100], Train Loss: 0.7180, Val Loss: 0.6848\n",
      "Epoch [45/100], Train Loss: 0.6879, Val Loss: 0.6439\n",
      "Epoch [46/100], Train Loss: 0.6822, Val Loss: 0.6524\n",
      "Epoch [47/100], Train Loss: 0.6554, Val Loss: 0.7459\n",
      "Epoch [48/100], Train Loss: 0.6362, Val Loss: 0.6114\n",
      "Epoch [49/100], Train Loss: 0.6282, Val Loss: 0.5969\n",
      "Epoch [50/100], Train Loss: 0.6190, Val Loss: 0.7532\n",
      "Epoch [51/100], Train Loss: 0.6080, Val Loss: 0.5708\n",
      "Epoch [52/100], Train Loss: 0.5947, Val Loss: 0.6623\n",
      "Epoch [53/100], Train Loss: 0.5961, Val Loss: 0.5847\n",
      "Epoch [54/100], Train Loss: 0.5805, Val Loss: 0.5591\n",
      "Epoch [55/100], Train Loss: 0.5745, Val Loss: 0.5994\n",
      "Epoch [56/100], Train Loss: 0.5629, Val Loss: 0.5351\n",
      "Epoch [57/100], Train Loss: 0.5639, Val Loss: 0.6835\n",
      "Epoch [58/100], Train Loss: 0.5532, Val Loss: 0.5640\n",
      "Epoch [59/100], Train Loss: 0.5472, Val Loss: 0.6052\n",
      "Epoch [60/100], Train Loss: 0.5405, Val Loss: 0.7218\n",
      "Epoch [61/100], Train Loss: 0.5397, Val Loss: 0.5308\n",
      "Epoch [62/100], Train Loss: 0.5371, Val Loss: 0.4993\n",
      "Epoch [63/100], Train Loss: 0.5299, Val Loss: 0.5059\n",
      "Epoch [64/100], Train Loss: 0.5295, Val Loss: 0.5624\n",
      "Epoch [65/100], Train Loss: 0.5195, Val Loss: 0.6087\n",
      "Epoch [66/100], Train Loss: 0.5170, Val Loss: 0.5808\n",
      "Epoch [67/100], Train Loss: 0.5221, Val Loss: 0.5860\n",
      "Epoch [68/100], Train Loss: 0.5152, Val Loss: 0.5515\n",
      "Epoch [69/100], Train Loss: 0.5147, Val Loss: 0.5361\n",
      "Epoch [70/100], Train Loss: 0.5085, Val Loss: 0.5061\n",
      "Epoch [71/100], Train Loss: 0.5084, Val Loss: 0.5950\n",
      "Epoch [72/100], Train Loss: 0.5029, Val Loss: 0.4898\n",
      "Epoch [73/100], Train Loss: 0.5007, Val Loss: 0.4816\n",
      "Epoch [74/100], Train Loss: 0.4961, Val Loss: 0.5017\n",
      "Epoch [75/100], Train Loss: 0.5029, Val Loss: 0.5297\n",
      "Epoch [76/100], Train Loss: 0.4998, Val Loss: 0.5780\n",
      "Epoch [77/100], Train Loss: 0.4994, Val Loss: 0.4995\n",
      "Epoch [78/100], Train Loss: 0.4938, Val Loss: 0.4717\n",
      "Epoch [79/100], Train Loss: 0.4884, Val Loss: 0.5107\n",
      "Epoch [80/100], Train Loss: 0.4899, Val Loss: 0.5130\n",
      "Epoch [81/100], Train Loss: 0.4858, Val Loss: 0.4766\n",
      "Epoch [82/100], Train Loss: 0.4836, Val Loss: 0.4737\n",
      "Epoch [83/100], Train Loss: 0.4788, Val Loss: 0.5114\n",
      "Epoch [84/100], Train Loss: 0.4807, Val Loss: 0.4889\n",
      "Epoch [85/100], Train Loss: 0.4835, Val Loss: 0.4700\n",
      "Epoch [86/100], Train Loss: 0.4862, Val Loss: 0.4839\n",
      "Epoch [87/100], Train Loss: 0.4788, Val Loss: 0.5398\n",
      "Epoch [88/100], Train Loss: 0.4789, Val Loss: 0.4583\n",
      "Epoch [89/100], Train Loss: 0.4773, Val Loss: 0.4557\n",
      "Epoch [90/100], Train Loss: 0.4721, Val Loss: 0.4538\n",
      "Epoch [91/100], Train Loss: 0.4730, Val Loss: 0.4624\n",
      "Epoch [92/100], Train Loss: 0.4745, Val Loss: 0.5310\n",
      "Epoch [93/100], Train Loss: 0.4725, Val Loss: 0.4573\n",
      "Epoch [94/100], Train Loss: 0.4678, Val Loss: 0.4805\n",
      "Epoch [95/100], Train Loss: 0.4694, Val Loss: 0.4530\n",
      "Epoch [96/100], Train Loss: 0.4685, Val Loss: 0.4749\n",
      "Epoch [97/100], Train Loss: 0.4742, Val Loss: 0.4537\n",
      "Epoch [98/100], Train Loss: 0.4652, Val Loss: 0.5888\n",
      "Epoch [99/100], Train Loss: 0.4600, Val Loss: 0.5827\n",
      "Epoch [100/100], Train Loss: 0.4601, Val Loss: 0.4478\n",
      "Test RMSE: 0.6688329577445984\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets and loaders\n",
    "train_dataset = TensorDataset(train_features_tensor, train_targets_tensor)\n",
    "val_dataset = TensorDataset(val_features_tensor, val_targets_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_targets_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = VanillaNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Logging training and validation loss\n",
    "    print(f'Epoch [{epoch+1}/{100}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in DataLoader(dataset=test_dataset, batch_size=64, shuffle=False):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(DataLoader(dataset=test_dataset, batch_size=64))\n",
    "test_rmse = torch.sqrt(torch.tensor(avg_test_loss))\n",
    "print(f'Test RMSE: {test_rmse.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LR: 0.001 and Optimizer: Adam\n",
      "Validation RMSE: 0.7278169989585876\n",
      "Training with LR: 0.001 and Optimizer: SGD\n",
      "Validation RMSE: 1.7770209312438965\n",
      "Training with LR: 0.01 and Optimizer: Adam\n",
      "Validation RMSE: 0.7353687882423401\n",
      "Training with LR: 0.01 and Optimizer: SGD\n",
      "Validation RMSE: 7.022190093994141\n",
      "Training with LR: 0.1 and Optimizer: Adam\n",
      "Validation RMSE: 2.38685941696167\n",
      "Training with LR: 0.1 and Optimizer: SGD\n",
      "Validation RMSE: 7.022299766540527\n",
      "Best Learning Rate: 0.001, Best Optimizer: Adam, Best RMSE: 0.7278169989585876\n"
     ]
    }
   ],
   "source": [
    "best_rmse = float('inf')\n",
    "best_lr = None\n",
    "best_optimizer_type = None\n",
    "\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    for optimizer_type in [optim.Adam, optim.SGD]:\n",
    "        model = VanillaNet()\n",
    "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
    "\n",
    "        print(f\"Training with LR: {lr} and Optimizer: {'Adam' if optimizer_type == optim.Adam else 'SGD'}\")\n",
    "        for epoch in range(100):  # 100 epochs for tuning\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.view(-1, 1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_features_tensor)\n",
    "            val_loss = torch.sqrt(criterion(val_outputs, val_targets_tensor.view(-1, 1)))\n",
    "            val_rmse = val_loss.item()\n",
    "        print(f\"Validation RMSE: {val_rmse}\")\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_lr = lr\n",
    "            best_optimizer_type = optimizer_type\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "print(f'Best Learning Rate: {best_lr}, Best Optimizer: {best_optimizer_type.__name__}, Best RMSE: {best_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE with Best Model: 0.7311059832572937\n"
     ]
    }
   ],
   "source": [
    "tuned_model = VanillaNet()\n",
    "tuned_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "tuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = tuned_model(test_features_tensor)\n",
    "    test_loss = torch.sqrt(criterion(test_outputs, test_targets_tensor.view(-1, 1)))\n",
    "    print(f'Test RMSE with Best Model: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning our vanilla feedforward neural network to a learning rate of 0.001 and the Adam optimizer, we again found a very minimal RMSE of 0.727 which became 0.7311 on the test set. The vanilla feedforward neural net does about as well as the SparkML random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(273, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 75.3744, Val Loss: 3.3501\n",
      "Epoch [2/100], Train Loss: 2.6339, Val Loss: 2.3258\n",
      "Epoch [3/100], Train Loss: 2.1581, Val Loss: 1.9718\n",
      "Epoch [4/100], Train Loss: 2.0068, Val Loss: 1.8178\n",
      "Epoch [5/100], Train Loss: 1.8927, Val Loss: 1.6306\n",
      "Epoch [6/100], Train Loss: 1.6628, Val Loss: 2.9055\n",
      "Epoch [7/100], Train Loss: 1.5502, Val Loss: 1.8685\n",
      "Epoch [8/100], Train Loss: 1.4828, Val Loss: 1.6158\n",
      "Epoch [9/100], Train Loss: 1.4611, Val Loss: 1.2947\n",
      "Epoch [10/100], Train Loss: 1.4074, Val Loss: 1.8310\n",
      "Epoch [11/100], Train Loss: 1.3814, Val Loss: 1.7881\n",
      "Epoch [12/100], Train Loss: 1.3105, Val Loss: 1.1843\n",
      "Epoch [13/100], Train Loss: 1.2568, Val Loss: 1.2222\n",
      "Epoch [14/100], Train Loss: 1.2084, Val Loss: 1.5930\n",
      "Epoch [15/100], Train Loss: 1.1798, Val Loss: 1.1070\n",
      "Epoch [16/100], Train Loss: 1.1220, Val Loss: 1.1322\n",
      "Epoch [17/100], Train Loss: 1.0807, Val Loss: 0.9338\n",
      "Epoch [18/100], Train Loss: 1.0299, Val Loss: 1.1811\n",
      "Epoch [19/100], Train Loss: 0.9712, Val Loss: 0.9122\n",
      "Epoch [20/100], Train Loss: 0.9109, Val Loss: 0.8523\n",
      "Epoch [21/100], Train Loss: 0.9172, Val Loss: 1.0198\n",
      "Epoch [22/100], Train Loss: 0.8544, Val Loss: 0.8020\n",
      "Epoch [23/100], Train Loss: 0.8270, Val Loss: 0.7344\n",
      "Epoch [24/100], Train Loss: 0.8300, Val Loss: 0.7713\n",
      "Epoch [25/100], Train Loss: 0.7754, Val Loss: 0.7358\n",
      "Epoch [26/100], Train Loss: 0.7592, Val Loss: 0.7709\n",
      "Epoch [27/100], Train Loss: 0.7328, Val Loss: 0.7227\n",
      "Epoch [28/100], Train Loss: 0.7115, Val Loss: 0.8488\n",
      "Epoch [29/100], Train Loss: 0.7076, Val Loss: 0.7748\n",
      "Epoch [30/100], Train Loss: 0.7098, Val Loss: 0.8546\n",
      "Epoch [31/100], Train Loss: 0.6888, Val Loss: 0.6133\n",
      "Epoch [32/100], Train Loss: 0.6666, Val Loss: 0.6776\n",
      "Epoch [33/100], Train Loss: 0.6906, Val Loss: 0.6109\n",
      "Epoch [34/100], Train Loss: 0.6629, Val Loss: 0.6372\n",
      "Epoch [35/100], Train Loss: 0.6528, Val Loss: 0.7268\n",
      "Epoch [36/100], Train Loss: 0.6510, Val Loss: 0.6046\n",
      "Epoch [37/100], Train Loss: 0.6416, Val Loss: 0.5619\n",
      "Epoch [38/100], Train Loss: 0.6327, Val Loss: 0.7615\n",
      "Epoch [39/100], Train Loss: 0.6337, Val Loss: 0.6287\n",
      "Epoch [40/100], Train Loss: 0.6101, Val Loss: 1.2522\n",
      "Epoch [41/100], Train Loss: 0.6321, Val Loss: 0.8907\n",
      "Epoch [42/100], Train Loss: 0.6050, Val Loss: 0.6907\n",
      "Epoch [43/100], Train Loss: 0.6127, Val Loss: 0.5419\n",
      "Epoch [44/100], Train Loss: 0.5994, Val Loss: 0.6330\n",
      "Epoch [45/100], Train Loss: 0.5924, Val Loss: 0.5560\n",
      "Epoch [46/100], Train Loss: 0.5801, Val Loss: 1.0048\n",
      "Epoch [47/100], Train Loss: 0.5948, Val Loss: 0.5893\n",
      "Epoch [48/100], Train Loss: 0.5943, Val Loss: 0.9947\n",
      "Epoch [49/100], Train Loss: 0.5671, Val Loss: 1.0374\n",
      "Epoch [50/100], Train Loss: 0.5624, Val Loss: 0.5187\n",
      "Epoch [51/100], Train Loss: 0.5740, Val Loss: 0.5262\n",
      "Epoch [52/100], Train Loss: 0.5624, Val Loss: 0.5859\n",
      "Epoch [53/100], Train Loss: 0.5568, Val Loss: 0.7208\n",
      "Epoch [54/100], Train Loss: 0.5445, Val Loss: 0.5557\n",
      "Epoch [55/100], Train Loss: 0.5610, Val Loss: 0.6011\n",
      "Epoch [56/100], Train Loss: 0.5449, Val Loss: 0.6038\n",
      "Epoch [57/100], Train Loss: 0.5422, Val Loss: 0.5067\n",
      "Epoch [58/100], Train Loss: 0.5413, Val Loss: 0.6395\n",
      "Epoch [59/100], Train Loss: 0.5321, Val Loss: 0.6851\n",
      "Epoch [60/100], Train Loss: 0.5328, Val Loss: 0.5695\n",
      "Epoch [61/100], Train Loss: 0.5378, Val Loss: 0.5041\n",
      "Epoch [62/100], Train Loss: 0.5283, Val Loss: 0.5581\n",
      "Epoch [63/100], Train Loss: 0.5332, Val Loss: 0.7303\n",
      "Epoch [64/100], Train Loss: 0.5265, Val Loss: 0.6102\n",
      "Epoch [65/100], Train Loss: 0.5361, Val Loss: 0.5085\n",
      "Epoch [66/100], Train Loss: 0.5260, Val Loss: 0.5503\n",
      "Epoch [67/100], Train Loss: 0.5253, Val Loss: 0.4940\n",
      "Epoch [68/100], Train Loss: 0.5178, Val Loss: 0.5370\n",
      "Epoch [69/100], Train Loss: 0.5247, Val Loss: 0.4704\n",
      "Epoch [70/100], Train Loss: 0.5249, Val Loss: 0.4957\n",
      "Epoch [71/100], Train Loss: 0.5237, Val Loss: 0.6134\n",
      "Epoch [72/100], Train Loss: 0.5257, Val Loss: 0.5553\n",
      "Epoch [73/100], Train Loss: 0.5101, Val Loss: 0.4642\n",
      "Epoch [74/100], Train Loss: 0.5276, Val Loss: 0.4895\n",
      "Epoch [75/100], Train Loss: 0.5034, Val Loss: 0.5731\n",
      "Epoch [76/100], Train Loss: 0.5096, Val Loss: 0.4870\n",
      "Epoch [77/100], Train Loss: 0.5185, Val Loss: 0.5008\n",
      "Epoch [78/100], Train Loss: 0.5066, Val Loss: 0.4952\n",
      "Epoch [79/100], Train Loss: 0.5091, Val Loss: 0.8300\n",
      "Epoch [80/100], Train Loss: 0.5077, Val Loss: 0.5030\n",
      "Epoch [81/100], Train Loss: 0.5021, Val Loss: 0.4709\n",
      "Epoch [82/100], Train Loss: 0.5064, Val Loss: 0.5252\n",
      "Epoch [83/100], Train Loss: 0.4996, Val Loss: 0.5719\n",
      "Epoch [84/100], Train Loss: 0.4975, Val Loss: 0.4565\n",
      "Epoch [85/100], Train Loss: 0.4990, Val Loss: 0.6336\n",
      "Epoch [86/100], Train Loss: 0.4998, Val Loss: 0.5008\n",
      "Epoch [87/100], Train Loss: 0.4933, Val Loss: 0.4603\n",
      "Epoch [88/100], Train Loss: 0.4950, Val Loss: 0.5711\n",
      "Epoch [89/100], Train Loss: 0.4936, Val Loss: 0.5035\n",
      "Epoch [90/100], Train Loss: 0.4977, Val Loss: 0.4505\n",
      "Epoch [91/100], Train Loss: 0.5000, Val Loss: 0.6086\n",
      "Epoch [92/100], Train Loss: 0.5053, Val Loss: 0.4711\n",
      "Epoch [93/100], Train Loss: 0.4860, Val Loss: 0.6238\n",
      "Epoch [94/100], Train Loss: 0.4850, Val Loss: 0.5320\n",
      "Epoch [95/100], Train Loss: 0.4894, Val Loss: 0.5717\n",
      "Epoch [96/100], Train Loss: 0.4950, Val Loss: 0.5328\n",
      "Epoch [97/100], Train Loss: 0.4878, Val Loss: 0.7065\n",
      "Epoch [98/100], Train Loss: 0.4880, Val Loss: 0.5407\n",
      "Epoch [99/100], Train Loss: 0.4836, Val Loss: 0.5844\n",
      "Epoch [100/100], Train Loss: 0.4810, Val Loss: 0.4708\n",
      "Test RMSE: 0.6875653862953186\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{100}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in DataLoader(dataset=test_dataset, batch_size=64, shuffle=False):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(DataLoader(dataset=test_dataset, batch_size=64))\n",
    "test_rmse = torch.sqrt(torch.tensor(avg_test_loss))\n",
    "print(f'Test RMSE: {test_rmse.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LR: 0.001 and Optimizer: Adam\n",
      "Validation RMSE: 0.7242836356163025\n",
      "Training with LR: 0.001 and Optimizer: SGD\n",
      "Validation RMSE: 7.0218586921691895\n",
      "Training with LR: 0.01 and Optimizer: Adam\n",
      "Validation RMSE: 0.8764960765838623\n",
      "Training with LR: 0.01 and Optimizer: SGD\n",
      "Validation RMSE: nan\n",
      "Training with LR: 0.1 and Optimizer: Adam\n",
      "Validation RMSE: 7.022477626800537\n",
      "Training with LR: 0.1 and Optimizer: SGD\n",
      "Validation RMSE: nan\n",
      "Best Learning Rate: 0.001, Best Optimizer: Adam, Best RMSE: 0.7242836356163025\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "best_rmse = float('inf')\n",
    "best_lr = None\n",
    "best_optimizer_type = None\n",
    "\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    for optimizer_type in [optim.Adam, optim.SGD]:\n",
    "        model = MLP()\n",
    "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
    "\n",
    "        print(f\"Training with LR: {lr} and Optimizer: {'Adam' if optimizer_type == optim.Adam else 'SGD'}\")\n",
    "        for epoch in range(100):  # 50 epochs for tuning\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.view(-1, 1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_features_tensor)\n",
    "            val_loss = torch.sqrt(criterion(val_outputs, val_targets_tensor.view(-1, 1)))\n",
    "            val_rmse = val_loss.item()\n",
    "        print(f\"Validation RMSE: {val_rmse}\")\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_lr = lr\n",
    "            best_optimizer_type = optimizer_type\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "print(f'Best Learning Rate: {best_lr}, Best Optimizer: {best_optimizer_type.__name__}, Best RMSE: {best_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE with Best Model: 0.7273406386375427\n"
     ]
    }
   ],
   "source": [
    "tuned_model = MLP()\n",
    "tuned_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "tuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = tuned_model(test_features_tensor)\n",
    "    test_loss = torch.sqrt(criterion(test_outputs, test_targets_tensor.view(-1, 1)))\n",
    "    print(f'Test RMSE with Best Model: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after tuning our custom MLP model, we found the same parameters to be effective (a learning rate of 0.001 and an Adam optimizer), which gave extremely similar resutls with an RMSE of 0.724. When run against the test set, we see that error remain the same aroud 0.727 approximately. With that the models ranked by performance are 1. MLP, 2. Feedforward, 3. Random Forest, 4. Linear Regression. All models achieved a sub 2 RMSE after hyperparamter tuning which is quite robust and effective for this supervised regression problem. It seems like the dataset has been cleaned, preprocessed, and normalized quite well to be able to predict the overall rating of a FIFA player!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
